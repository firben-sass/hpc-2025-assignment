Loaded module: gprofng/2.43.1
Creating experiment directory collector.23785728.1.er (Process ID: 759107) ...
Used 10 iterations, diff = 231616.608972, time taken: 0.15896496
Creating experiment directory collector.23785728.2.er (Process ID: 759114) ...
Used 10 iterations, diff = 231616.608972, time taken: 0.16551219
Creating experiment directory collector.23785728.3.er (Process ID: 759117) ...
Used 10 iterations, diff = 231616.608972, time taken: 0.16253369
Creating experiment directory collector.23785728.6.er (Process ID: 759121) ...
Used 10 iterations, diff = 231616.608972, time taken: 0.16477407
Creating experiment directory collector.23785728.12.er (Process ID: 759129) ...
Used 10 iterations, diff = 231616.608972, time taken: 0.16539661
Creating experiment directory collector.23785728.24.er (Process ID: 759143) ...
Used 10 iterations, diff = 231616.608972, time taken: 0.16401667

------------------------------------------------------------
Sender: LSF System <lsfadmin@hpc.dtu.dk>
Subject: Job 23785728: <collector> in cluster <dcc> Done

Job <collector> was submitted from host <n-62-30-3> by user <s204164> in cluster <dcc> at Fri Jan 17 17:53:46 2025
Job was executed on host(s) <n-62-21-77>, in queue <hpcintro>, as user <s204164> in cluster <dcc> at Fri Jan 17 17:53:47 2025
</zhome/5f/3/156515> was used as the home directory.
</zhome/5f/3/156515/Documents/hpc-2025-assignment> was used as the working directory.
Started at Fri Jan 17 17:53:47 2025
Terminated at Fri Jan 17 17:54:20 2025
Results reported at Fri Jan 17 17:54:20 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/bash
# 02614 - High-Performance Computing, January 2024
# 
# batch script to run gprofng collect on a decidated server in the hpcintro
# queue
#
# Author: Bernd Dammann <bd@cc.dtu.dk>
#
#BSUB -J collector
#BSUB -o collector_%J.out
#BSUB -q hpcintro
#BSUB -n 1
#BSUB -R "rusage[mem=2048]"
#BSUB -W 15
# uncomment the following line, if you want to assure that your job has
# a whole CPU for itself (shared L3 cache)
# #BSUB -R "span[hosts=1] affinity[socket(1)]"
#BSUB -R "select[model == XeonE5_2650v4]"

# needed for the collect tool
module load gprofng

# define the driver name to use
# valid values: matmult_c.studio, matmult_f.studio, matmult_c.gcc or
# matmult_f.gcc
#
EXECUTABLE=poisson_j
THREADS=(1 2 3 6 12 24)

# define the max no. of iterations the driver should use - adjust to
# get a reasonable run time.  You can get an estimate by trying this
# on the command line, i.e. "MFLOPS_MAX_IT=10 ./matmult_...." for the
# problem size you want to analyze.
#
export MFLOPS_MAX_IT=1000

# loop over each thread count
for THREAD in "${THREADS[@]}"; do
  export OMP_NUM_THREADS=${THREAD}

  # experiment name
  JID=${LSB_JOBID}
  EXPOUT="$LSB_JOBNAME.${JID}.${THREAD}.er"

  # uncomment the HWCOUNT line, if you want to use hardware counters
  HWCOUNT="-h dch,on,dcm,on,l2h,on,l2m,on"

  # start the collect command with the above settings
  gprofng collect app -o $EXPOUT $HWCOUNT ./$EXECUTABLE 300 10 0 0 0
done

(... more ...)
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   31.55 sec.
    Max Memory :                                 5 MB
    Average Memory :                             5.00 MB
    Total Requested Memory :                     2048.00 MB
    Delta Memory :                               2043.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                5
    Run time :                                   33 sec.
    Turnaround time :                            34 sec.

The output (if any) is above this job summary.

