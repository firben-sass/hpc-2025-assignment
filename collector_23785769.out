Loaded module: gprofng/2.43.1
Creating experiment directory collector.23785769.1.er (Process ID: 259916) ...
Used 10 iterations, diff = 231616.608972, time taken: 1.62305342
Creating experiment directory collector.23785769.24.er (Process ID: 259921) ...
Used 10 iterations, diff = 231616.608972, time taken: 1.67332592

------------------------------------------------------------
Sender: LSF System <lsfadmin@hpc.dtu.dk>
Subject: Job 23785769: <collector> in cluster <dcc> Done

Job <collector> was submitted from host <n-62-30-3> by user <s204164> in cluster <dcc> at Fri Jan 17 17:59:41 2025
Job was executed on host(s) <n-62-21-6>, in queue <hpcintro>, as user <s204164> in cluster <dcc> at Fri Jan 17 17:59:42 2025
</zhome/5f/3/156515> was used as the home directory.
</zhome/5f/3/156515/Documents/hpc-2025-assignment> was used as the working directory.
Started at Fri Jan 17 17:59:42 2025
Terminated at Fri Jan 17 17:59:54 2025
Results reported at Fri Jan 17 17:59:54 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/bash
# 02614 - High-Performance Computing, January 2024
# 
# batch script to run gprofng collect on a decidated server in the hpcintro
# queue
#
# Author: Bernd Dammann <bd@cc.dtu.dk>
#
#BSUB -J collector
#BSUB -o collector_%J.out
#BSUB -q hpcintro
#BSUB -n 1
#BSUB -R "rusage[mem=2048]"
#BSUB -W 15
# uncomment the following line, if you want to assure that your job has
# a whole CPU for itself (shared L3 cache)
# #BSUB -R "span[hosts=1] affinity[socket(1)]"
#BSUB -R "select[model == XeonE5_2650v4]"

# needed for the collect tool
module load gprofng

# define the driver name to use
# valid values: matmult_c.studio, matmult_f.studio, matmult_c.gcc or
# matmult_f.gcc
#
EXECUTABLE=poisson_j
THREADS=("1" "24")

# define the max no. of iterations the driver should use - adjust to
# get a reasonable run time.  You can get an estimate by trying this
# on the command line, i.e. "MFLOPS_MAX_IT=10 ./matmult_...." for the
# problem size you want to analyze.
#
export MFLOPS_MAX_IT=1000

# loop over each thread count
for THREAD in "${THREADS[@]}"; do
  export OMP_NUM_THREADS=${THREAD}

  # experiment name
  JID=${LSB_JOBID}
  EXPOUT="$LSB_JOBNAME.${JID}.${THREAD}.er"

  # uncomment the HWCOUNT line, if you want to use hardware counters
  HWCOUNT="-h dch,on,dcm,on,l2h,on,l2m,on"

  # start the collect command with the above settings
  gprofng collect app -o $EXPOUT $HWCOUNT ./$EXECUTABLE 300 10 0 0 0
done

(... more ...)
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   11.06 sec.
    Max Memory :                                 57 MB
    Average Memory :                             57.00 MB
    Total Requested Memory :                     2048.00 MB
    Delta Memory :                               1991.00 MB
    Max Swap :                                   -
    Max Processes :                              5
    Max Threads :                                29
    Run time :                                   12 sec.
    Turnaround time :                            13 sec.

The output (if any) is above this job summary.

