#!/bin/bash
# 02614 - High-Performance Computing, January 2024
# 
# batch script to run gprofng collect on a decidated server in the hpcintro
# queue
#
# Author: Bernd Dammann <bd@cc.dtu.dk>
#
#BSUB -J collector
#BSUB -o collector_%J.out
#BSUB -q hpcintro
#BSUB -n 24
#BSUB -R "rusage[mem=2048]"
#BSUB -W 15
# uncomment the following line, if you want to assure that your job has
# a whole CPU for itself (shared L3 cache)
#BSUB -R "span[hosts=1] affinity[socket(1)]"
#BSUB -R "select[model == XeonE5_2650v4]"

# needed for the collect tool
module load gprofng

# define the driver name to use
# valid values: matmult_c.studio, matmult_f.studio, matmult_c.gcc or
# matmult_f.gcc
#
EXECUTABLE=poisson_j
THREADS=("1" "24")

# define the max no. of iterations the driver should use - adjust to
# get a reasonable run time.  You can get an estimate by trying this
# on the command line, i.e. "MFLOPS_MAX_IT=10 ./matmult_...." for the
# problem size you want to analyze.
#
export MFLOPS_MAX_IT=1000

# loop over each thread count
for THREAD in "${THREADS[@]}"; do
  export OMP_NUM_THREADS=${THREAD}
  echo "OMP_NUM_THREADS=${OMP_NUM_THREADS}"

  # experiment name
  JID=${LSB_JOBID}
  EXPOUT="$LSB_JOBNAME.${JID}.${THREAD}.er"

  # uncomment the HWCOUNT line, if you want to use hardware counters
  HWCOUNT="-h dch,on,dcm,on,l2h,on,l2m,on"

  # start the collect command with the above settings
  gprofng collect app -o $EXPOUT $HWCOUNT ./$EXECUTABLE 300 10 0 0 0
done
